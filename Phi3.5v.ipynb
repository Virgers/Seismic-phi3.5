{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import requests \n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoProcessor \n",
    "\n",
    "model_id = \"./\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") \n",
    "\n",
    "images = []\n",
    "placeholder = \"\"\n",
    "\n",
    "# Note: if OOM, you might consider reduce number of frames in this example.\n",
    "# for i in range(1,20):\n",
    "#     url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\" \n",
    "#     images.append(Image.open(requests.get(url, stream=True).raw))\n",
    "#     placeholder += f\"<|image_{i}|>\\n\"\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": placeholder+\"Summarize the deck of slides.\"},\n",
    "# ]\n",
    "\n",
    "image_path = '/home/dell/disk1/Jinlong/Kolors/scripts/outputs/sample_test.jpg'\n",
    "\n",
    "with Image.open(image_path) as img:\n",
    "    images.append(img)\n",
    "    \n",
    "print(type(images))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Summarize the deck of slides.\"},\n",
    "]\n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "  messages, \n",
    "  tokenize=False, \n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 1000, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "generate_ids = model.generate(**inputs, \n",
    "  eos_token_id=processor.tokenizer.eos_token_id, \n",
    "  **generation_args\n",
    ")\n",
    "\n",
    "# remove input tokens \n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, \n",
    "  skip_special_tokens=True, \n",
    "  clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
